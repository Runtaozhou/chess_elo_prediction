#!/bin/bash
#SBATCH --job-name=transforming_chess_data    # Job name
#SBATCH --output=/home/zrc3hc/Chess/slurm_errors_and_out/transforming_chess_data.out  # Standard output file
#SBATCH --error=/home/zrc3hc/Chess/slurm_errors_and_out/transforming_chess_data.err   # Standard error file
#SBATCH --nodes=1                             # Request 1 node
#SBATCH --ntasks=1                            # Request 1 task
#SBATCH --cpus-per-task=8                     # Request 8 CPU cores
#SBATCH --time=120:00:00                       # Set time limit
#SBATCH --partition=standard                  # Partition

# Load required modules
module purge
module load gcc/13.3.0
module load spark/3.4.1
module load miniforge

# Activate the Conda environment
source activate chess_env

# Verify Python and PySpark installation
which python
python -m pip list | grep pyspark

# Run the script with spark-submit
spark-submit /sfs/gpfs/tardis/home/zrc3hc/Chess/1.\ Preprocessing/transforming_chess_data.py
